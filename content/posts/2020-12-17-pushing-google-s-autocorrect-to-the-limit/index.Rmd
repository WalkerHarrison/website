---
title: Pushing Google's Autocorrect to the Limit
author: Walker Harrison
date: '2020-12-17'
draft: true
slug: []
categories: []
tags: []
lastmod: '2020-12-14T15:18:41-05:00'
layout: post
type: post
highlight: no
---

I only had a vague understanding of the word "endocrinology" when it popped up [in a recent article](https://www.nytimes.com/aponline/2020/05/21/world/europe/ap-virus-outbreak-russia-disparaging-doctors.html), so I Googled it. Or at least, I tried to. Instead I typed "wndicromoly," and was disappointed by the results, all due respect to Wendy Cromwell:

<center><img src="C:\Users\wharrison\Desktop\wndicromoly.PNG" height="200px" width="600px"/></center>

My expectation was that Google would either automatically redirect me to the right results or at least politely ask, "Did I mean: endocrinology?" So spoiled am I by autocorrect features on the web and in my phone that I was actually upset when Google couldn't decrypt a keyword in which about half of the characters were typos. 

But then I wondered: which of those typos had been the fatal blow? At what point had the word been mutated beyond recognition?

It helps to understand exactly how Google suggests corrections in the first place, as [explained here in a talk given by their VP of Engineering](https://www.youtube.com/watch?v=syKY8CrHkck#t=22m20s). When users misspell their initial searches and then immediately search again without the typos, Google is able to associate those two inputs as an error and a correction, respectively. In other words, so many users have detoured from their intended search that they've worn a path in the ground that Google will now rescue you from going down. 

<details open>
  <summary>R Code</summary>

Anyway, we start by defining both the `word` and the `typo` and calculating the [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) between them, which measures the minimum number of single-character edits (adds, deletions, or substitutions) to get from one string to another:

```{r, echo = FALSE, include = FALSE, message = FALSE, warning = FALSE}
library(secret)

my_vault <- "/Users/walkerharrison/Desktop/website/static/data/secret-vault.vault"
walkerharrison_private_key <- "/Users/walkerharrison/.ssh/blog_vault"

google_search_api_key <- get_secret("google_search_api_key", key = walkerharrison_private_key, vault = my_vault)
google_search_cx <- get_secret("google_search_cx", key = walkerharrison_private_key, vault = my_vault)

```

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(data.tree)
library(httr)
```

```{r}
word <- "endocrinology"
typo <- "wndicromoly"
distance <- adist(word, typo)
```

Next we find a way to simulate typos. We start with a `keyboard`, which is a projection of the traditional QWERTY keyboard onto a matrix (only letters for now, let's keep it simple), and a distance matrix `d` that holds the Euclidean distance between each pair of keys. For example, 'g' would be 1 away from 'f' but $\sqrt{2}$ away from 'y'. 

The function `fat_finger` takes a letter and randomly samples from the rest of the keyboard, with probability weighted by proximity. So, using 'g' again as our example, the most likely typos would be 'f', 't',' 'h',' and 'b', while very little probability would be given to distant letters like 'q' and 'p'.

```{r}
keyboard <- rbind(
  c('q', 'w', 'e', 'r', 't', 'y', 'u', 'i', 'o', 'p'),
  c('a', 's', 'd', 'f', 'g', 'h', 'j', 'k', 'l', NA),
  c('z', 'x', 'c', 'v', 'b', 'n', 'm', NA, NA, NA)
)

d <- as.matrix(dist(expand.grid(1:nrow(keyboard), 
                                1:ncol(keyboard))))

fat_finger <- function(letter){
  
  # determine possible errors
  idx <- keyboard != letter & !is.na(keyboard)
  elements <- keyboard[idx]
  distance <- d[which(keyboard == letter), idx]
  
  # sample based on proximity
  p <- exp(-distance)/sum(exp(-distance))
  sample(elements, size = 1, prob = p)
}
```

The `fat_finger` function is then used by the `random_edit` function to mutate a given word. Words can be modified in the three distinct fashions recognized by Levenshtein:

- deleting a letter
- adding a letter (split into adding before, and adding after)
- swapping one letter in for another

```{r}
random_edit <- function(word){
  
  # select where to put error
  len <- nchar(word)
  idx <- sample(1:len, 1)
  
  # select type of error
  type <- sample(1:4, 1)

  # delete
  if(type == 1) return(paste0(substr(word, 1, idx-1), 
                              substr(word, idx+1, len)))
  # add before
  if(type == 2) return(paste0(substr(word, 1, idx-1), 
                              fat_finger(substr(word, idx, idx)), 
                              substr(word, idx, len)))
  # add after
  if(type == 3) return(paste0(substr(word, 1, idx), 
                              fat_finger(substr(word, idx, idx)), 
                              substr(word, idx + 1, len)))
  # swap
  if(type == 4) return(paste0(substr(word, 1, idx-1), 
                              fat_finger(substr(word, idx, idx)),
                              substr(word, idx+1, nchar(word))))
}
```

Finally, we can string individual mutations into a chain of them with the `make_latter` function, which recursively modifies a `word` until it becomes an intended `typo`. More explicitly, at every iteration the `random_edit` function is called until it finds a `new` word that is closer to the ultimate typo, as measured by Levenshtein distance. 

```{r}
make_latter <- function(word, typo) {
  
  # original distance
  ldist <- as.vector(adist(word, typo))
  
  # escape recursion once word becomes intended typo
  if (ldist == 0) return(word)
  
  # otherwise keep mutating
  else{
    while(ldist >= as.vector(adist(word, typo))){
      new <- random_edit(word)
      ldist <- as.vector(adist(new, typo))
    }
    
    # chain together mutations
    return(c(word, make_latter(new, typo)))
  }
}
```

What does all this get us to? Well we can make a series of latters from our original `word` to the `typo`, with each element representing a single character update:

```{r}
set.seed(0)
n <- 10
latters <- map(1:n, ~make_latter(word, typo))
paste(latters[[1]], collapse = " --> ")
```

Now we have to search for these misspelled terms. In theory we could try converting each string into a google.com URL and then scraping the relevant information, but that approach comes with the [risk of having your IP address blocked](https://stackoverflow.com/a/22657917/7711632) once Google catches on.

So instead, we use Google's [Custom Search API](https://developers.google.com/custom-search/v1/introduction). The API is designed to look within specific websites, but [you can set it to search the entire web](https://stackoverflow.com/questions/4933097/how-can-i-search-the-entire-web-from-google-custom-search). Once we have an `api_key` and a `cx`, which identifies a specific custom search engine, we can wrap the functionality into a function `google`. 

Note that this API's free tier only allows 100 requests per day, and they can't all be within the same 100 seconds, so a 2-3 second `pause` is built into the function.

```{r, cache = TRUE, eval = FALSE}
google <- function(term, 
                   api_key = google_search_api_key, 
                   cx = google_search_cx,
                   pause = 2){
  
  url <- "https://www.googleapis.com/customsearch/v1"
  request <- GET(url, query = list(key = api_key, cx = cx, q = term))
  
  # pause for a moment to stay within API limits
  Sys.sleep(runif(1, pause, pause + 1))
  
  content(request)
}

# find unique words in list of latters and google them
terms <- unique(unlist(latters))
search_results <- map(terms, google)

```

Once we've (responsibly) retrieved the searches, we can traverse the JSON responses to determine which queries Google tried to amend, and what the suggested fixes actually were.

```{r, eval = FALSE}
# look for suggested fixes in each result
corrected <- search_results %>%
  map("spelling") %>%
  map("correctedQuery") %>%
  map(str_detect, word) %>%
  modify_if(is_empty, ~FALSE) %>%
  unlist()

corrected_df <- data.frame(term = terms, 
                           corrected = corrected) %>%
  mutate(corrected = corrected | term == word)

latters_df <- data.frame(
  step = rep(1:(distance+1), n),
  latter = rep(1:n, each = distance+1),
  word = unlist(latters)) %>%
  inner_join(corrected_df, by = c('word' = 'term'))
```

```{r, eval = FALSE}
latters_df %>%
  group_by(step, word) %>%
  mutate(y = max(latter)) %>%
  ggplot(aes(step, y, group = latter, label = word)) +
  geom_line() +
  geom_label(aes(color = corrected)) +
  theme_void() +
  theme(legend.position = 'none')
```

</details> 

```{r, eval = FALSE}
word <- "youtube"

make_tree <- function(word, depth, n_children){
  if(depth == 0) return()
  children <- map(1:n_children, ~random_edit(word))
  while(n_distinct(children) < n_children){
    children <- children %>%
      modify_at(which(duplicated(children)),
                ~random_edit(word))
  }
  return(map(set_names(children), ~make_tree(.x, depth-1, n_children)))
}

set.seed(1)
word_tree <- FromListSimple(make_tree(word, 5, 2))
word_tree$Set(name = modify_at(word_tree$Get("name"), 1, ~word))

terms <- word_tree$Get("name")

SetGraphStyle(word_tree, rankdir = "LR")
SetNodeStyle(word_tree, color = function(node){
  ifelse(str_detect(node$name, "n"), "#F8766D", "#00BFC4")
})

plot(word_tree)
```
